{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.agents import Tool, AgentExecutor, create_react_agent\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Set\n",
    "from pydantic import BaseModel, Field\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def load_skills_dictionary():\n",
    "    \"\"\"Load comprehensive skills dictionary\"\"\"\n",
    "    return {\n",
    "        # Technical Skills\n",
    "        'programming_languages': {\n",
    "            'python', 'java', 'javascript', 'c++', 'c#', 'ruby', 'php', 'swift', 'kotlin', \n",
    "            'golang', 'rust', 'typescript', 'scala', 'perl', 'r', 'matlab'\n",
    "        },\n",
    "        'web_technologies': {\n",
    "            'html', 'css', 'react', 'angular', 'vue.js', 'node.js', 'express.js', 'django',\n",
    "            'flask', 'spring', 'asp.net', 'jquery', 'bootstrap', 'sass', 'less'\n",
    "        },\n",
    "        'databases': {\n",
    "            'sql', 'mysql', 'postgresql', 'mongodb', 'oracle', 'redis', 'elasticsearch',\n",
    "            'cassandra', 'dynamodb', 'neo4j', 'graphql'\n",
    "        },\n",
    "        'cloud_platforms': {\n",
    "            'aws', 'azure', 'gcp', 'docker', 'kubernetes', 'terraform', 'jenkins',\n",
    "            'circleci', 'gitlab', 'heroku', 'digitalocean'\n",
    "        },\n",
    "        'ai_ml': {\n",
    "            'machine learning', 'deep learning', 'tensorflow', 'pytorch', 'keras',\n",
    "            'scikit-learn', 'nlp', 'computer vision', 'neural networks', 'ai'\n",
    "        },\n",
    "        # Soft Skills\n",
    "        'soft_skills': {\n",
    "            'leadership', 'communication', 'teamwork', 'problem solving',\n",
    "            'project management', 'time management', 'critical thinking',\n",
    "            'adaptability', 'creativity', 'emotional intelligence'\n",
    "        }\n",
    "    }\n",
    "\n",
    "class CandidateInfo(BaseModel):\n",
    "    full_name: str = Field(default=\"Not found\")\n",
    "    email: str = Field(default=\"Not found\")\n",
    "    phone: str = Field(default=\"Not found\")\n",
    "    current_position: str = Field(default=\"Not found\")\n",
    "    years_experience: str = Field(default=\"Not found\")\n",
    "    key_skills: List[str] = Field(default_factory=list)\n",
    "    education: str = Field(default=\"Not found\")\n",
    "\n",
    "class ResumeMatchingAgent:\n",
    "    def __init__(self, groq_api_key: str):\n",
    "        self.llm = ChatGroq(\n",
    "            temperature=0,\n",
    "            groq_api_key=groq_api_key,\n",
    "            model_name=\"mixtral-8x7b-32768\"\n",
    "        )\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "        )\n",
    "        self.text_splitter = CharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            separator=\"\\n\"\n",
    "        )\n",
    "        self.vector_store = None\n",
    "        self.resume_metadata = {}\n",
    "        self.processed_resumes = set()\n",
    "        self.skills_dict = load_skills_dictionary()\n",
    "        try:\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "        except:\n",
    "            os.system('python -m spacy download en_core_web_sm')\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "        self._setup_agent()\n",
    "\n",
    "\n",
    "    def _setup_agent(self):\n",
    "        tools = [\n",
    "            Tool(\n",
    "                name=\"Search Resumes\",\n",
    "                func=self.find_matching_resumes,\n",
    "                description=\"Search for resumes matching a job description and requirements\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"Get Candidate Info\",\n",
    "                func=self.get_candidate_info,\n",
    "                description=\"Get detailed information about a specific candidate\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"Analyze Skills Gap\",\n",
    "                func=self.analyze_skills_gap,\n",
    "                description=\"Analyze the skills gap between required skills and candidate skills\"\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        prompt = PromptTemplate.from_template(\"\"\"\n",
    "        You are a helpful HR assistant that helps analyze resumes and find the best candidates.\n",
    "        Your primary task is to identify and display the candidate's name first, followed by other relevant details.\n",
    "        You have access to the following tools:\n",
    "\n",
    "        {tools}\n",
    "\n",
    "        Use the following format:\n",
    "\n",
    "        Question: the input question you must answer\n",
    "        Thought: you should always think about what to do\n",
    "        Action: the action to take, should be one of [{tool_names}]\n",
    "        Action Input: the input to the action\n",
    "        Observation: the result of the action\n",
    "        ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "        Thought: I now know the final answer\n",
    "        Final Answer: The candidate's name is [Candidate Name]. [Additional details based on the query].\n",
    "\n",
    "        Begin!\n",
    "\n",
    "        Question: {input}\n",
    "        {agent_scratchpad}\n",
    "        \"\"\")\n",
    "\n",
    "        agent = create_react_agent(self.llm, tools, prompt)\n",
    "        self.agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "    def load_resumes(self, resume_dir: str) -> List:\n",
    "        if not os.path.exists(resume_dir):\n",
    "            raise ValueError(f\"Resume directory '{resume_dir}' does not exist.\")\n",
    "\n",
    "        documents = []\n",
    "        unique_candidates = {}  # Track unique candidates by email/phone\n",
    "\n",
    "        for filename in os.listdir(resume_dir):\n",
    "            if filename.endswith(\".pdf\"):\n",
    "                if filename in self.processed_resumes:\n",
    "                    print(f\"Skipping duplicate resume file: {filename}\")\n",
    "                    continue\n",
    "\n",
    "                file_path = os.path.join(resume_dir, filename)\n",
    "                try:\n",
    "                    print(f\"Loading resume: {filename}\")\n",
    "                    loader = PyPDFLoader(file_path)\n",
    "                    docs = loader.load()\n",
    "                    candidate_info = self._extract_candidate_info(docs)\n",
    "\n",
    "                    # Create unique identifier for candidate\n",
    "                    candidate_id = f\"{candidate_info['Email']}_{candidate_info['Phone']}\"\n",
    "\n",
    "                    if candidate_id in unique_candidates:\n",
    "                        print(f\"Duplicate candidate found in {filename}. Using first occurrence.\")\n",
    "                        continue\n",
    "\n",
    "                    # Store unique candidate\n",
    "                    unique_candidates[candidate_id] = True\n",
    "                    documents.extend(docs)\n",
    "                    self.resume_metadata[filename] = {\n",
    "                        'candidate_info': candidate_info\n",
    "                    }\n",
    "                    self.processed_resumes.add(filename)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading resume '{filename}': {str(e)}\")\n",
    "\n",
    "        if documents:\n",
    "            self.create_vector_store(documents)\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def _extract_candidate_info(self, docs: List) -> Dict:\n",
    "        candidate_info = {\n",
    "            'Full Name': 'Not found',\n",
    "            'Email': 'Not found',\n",
    "            'Phone': 'Not found',\n",
    "            'Current Position': 'Not found',\n",
    "            'Years of Experience': 'Not found',\n",
    "            'Key Skills': [],\n",
    "            'Education': 'Not found'\n",
    "        }\n",
    "\n",
    "        full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "        # Improved name extraction\n",
    "        # Common headers that might appear above or before names\n",
    "        name_headers = [\"name:\", \"candidate:\", \"profile:\", \"curriculum vitae:\", \"resume:\", \"cv:\"]\n",
    "        # Words that typically don't appear in names\n",
    "        non_name_words = {\n",
    "            \"email\", \"phone\", \"skills\", \"experience\", \"education\", \"linkedin\", \"github\", \n",
    "            \"resume\", \"cv\", \"address\", \"summary\", \"objective\", \"profile\", \"contact\",\n",
    "            \"professional\", \"career\", \"tel\", \"mobile\", \"www\", \"http\", \"https\",\n",
    "            \"university\", \"college\", \"institute\", \"school\", \"academy\"  # Added educational keywords\n",
    "        }\n",
    "\n",
    "        # First try: Look for name after common headers\n",
    "        name_found = False\n",
    "        lines = [line.strip() for line in full_text.split('\\n') if line.strip()]\n",
    "\n",
    "        for i, line in enumerate(lines[:10]):  # Check first 10 lines\n",
    "            line_lower = line.lower()\n",
    "\n",
    "            # Check if line contains a header\n",
    "            if any(header in line_lower for header in name_headers):\n",
    "                # Get the text after the header\n",
    "                for header in name_headers:\n",
    "                    if header in line_lower:\n",
    "                        potential_name = line_lower.split(header)[1].strip()\n",
    "                        if potential_name:\n",
    "                            # Clean and verify the name\n",
    "                            cleaned_name = ' '.join(word.strip() for word in potential_name.split())\n",
    "                            if (len(cleaned_name.split()) >= 2 and  # At least two words\n",
    "                                not any(word.lower() in non_name_words for word in cleaned_name.split()) and\n",
    "                                cleaned_name[0].isalpha()):  # Starts with a letter\n",
    "                                candidate_info['Full Name'] = cleaned_name.title()\n",
    "                                name_found = True\n",
    "                                break\n",
    "                if name_found:\n",
    "                    break\n",
    "\n",
    "        # Second try: Look for a name pattern in the first few lines\n",
    "        if not name_found:\n",
    "            for line in lines[:5]:  # Check first 5 lines\n",
    "                # Skip lines with common non-name indicators\n",
    "                if any(word in line.lower() for word in non_name_words):\n",
    "                    continue\n",
    "\n",
    "                # Check if line matches name pattern:\n",
    "                # - 2-4 words\n",
    "                # - Each word starts with capital letter\n",
    "                # - No special characters except hyphen and apostrophe\n",
    "                words = line.split()\n",
    "                if (2 <= len(words) <= 4 and\n",
    "                    all(word[0].isupper() for word in words) and\n",
    "                    all(word.replace('-', '').replace(\"'\", '').isalnum() for word in words)):\n",
    "                    candidate_info['Full Name'] = line\n",
    "                    name_found = True\n",
    "                    break\n",
    "\n",
    "        # Third try: Use simple pattern matching for remaining cases\n",
    "        if not name_found:\n",
    "            name_pattern = r'^([A-Z][a-zA-Z\\'-]+\\s+[A-Z][a-zA-Z\\'-]+(?:\\s+[A-Z][a-zA-Z\\'-]+)?(?:\\s+[A-Z][a-zA-Z\\'-]+)?)$'\n",
    "            for line in lines[:10]:\n",
    "                if re.match(name_pattern, line):\n",
    "                    candidate_info['Full Name'] = line\n",
    "                    break\n",
    "\n",
    "        # Improved regex pattern for extracting emails\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b'\n",
    "\n",
    "        # Find all email matches in the text\n",
    "        emails = re.findall(email_pattern, full_text)\n",
    "\n",
    "        # Filter and select the most likely candidate email\n",
    "        if emails:\n",
    "            # Prioritize emails with common domains (e.g., gmail.com, yahoo.com, etc.)\n",
    "            common_domains = ['gmail', 'yahoo', 'outlook', 'hotmail', 'icloud']\n",
    "            for email in emails:\n",
    "                domain = email.split('@')[1].split('.')[0]  # Extract domain part\n",
    "                if domain.lower() in common_domains:\n",
    "                    candidate_info['Email'] = email\n",
    "                    break\n",
    "            else:\n",
    "                # If no common domain is found, use the first email\n",
    "                candidate_info['Email'] = emails[0]\n",
    "\n",
    "        # Extract phone\n",
    "        phone_pattern = r'\\b(?:\\+?1[-.]?)?\\s*(?:\\([0-9]{3}\\)|[0-9]{3})[-.]?[0-9]{3}[-.]?[0-9]{4}\\b'\n",
    "        phones = re.findall(phone_pattern, full_text)\n",
    "        if phones:\n",
    "            candidate_info['Phone'] = phones[0]\n",
    "\n",
    "        # Extract skills\n",
    "        skills_section = ''\n",
    "        for header in ['Skills:', 'Technical Skills:', 'Core Competencies:', 'Key Skills:']:\n",
    "            if header in full_text:\n",
    "                try:\n",
    "                    skills_section = full_text.split(header)[1].split('\\n\\n')[0]\n",
    "                    break\n",
    "                except IndexError:\n",
    "                    continue\n",
    "\n",
    "        if skills_section:\n",
    "            skills = re.split(r'[,•|\\n]', skills_section)\n",
    "            candidate_info['Key Skills'] = [skill.strip() for skill in skills if skill.strip()]\n",
    "\n",
    "        # Extract current position\n",
    "        position_keywords = [\"Current Position:\", \"Current Role:\", \"Present:\", \"Role:\"]\n",
    "        for keyword in position_keywords:\n",
    "            if keyword in full_text:\n",
    "                try:\n",
    "                    position_section = full_text.split(keyword)[1].split('\\n')[0].strip()\n",
    "                    candidate_info['Current Position'] = position_section\n",
    "                    break\n",
    "                except IndexError:\n",
    "                    continue\n",
    "\n",
    "        # Extract years of experience\n",
    "        experience_pattern = r'\\b(\\d+)\\s*(years?|yrs?)\\b'\n",
    "        experience_match = re.search(experience_pattern, full_text, re.IGNORECASE)\n",
    "        if experience_match:\n",
    "            candidate_info['Years of Experience'] = f\"{experience_match.group(1)} years\"\n",
    "\n",
    "        # Extract education\n",
    "        education_keywords = [\"Education:\", \"Academic Background:\", \"Degree:\"]\n",
    "        for keyword in education_keywords:\n",
    "            if keyword in full_text:\n",
    "                try:\n",
    "                    education_section = full_text.split(keyword)[1].split('\\n\\n')[0].strip()\n",
    "                    candidate_info['Education'] = education_section\n",
    "                    break\n",
    "                except IndexError:\n",
    "                    continue\n",
    "\n",
    "        return candidate_info\n",
    "\n",
    "    def create_vector_store(self, documents: List):\n",
    "        if not documents:\n",
    "            raise ValueError(\"No documents provided to create vector store.\")\n",
    "\n",
    "        self.vector_store = FAISS.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=self.embeddings\n",
    "        )\n",
    "\n",
    "    def find_matching_resumes(self, job_description: str, requirements: List[str]) -> List[Dict]:\n",
    "        if not self.vector_store:\n",
    "            raise ValueError(\"Vector store not initialized. Please load resumes first.\")\n",
    "\n",
    "        # Extract required skills from job description\n",
    "        required_skills = self._extract_skills_from_text(job_description)\n",
    "        \n",
    "        results = self.vector_store.similarity_search_with_score(job_description)\n",
    "        matches = []\n",
    "        seen_candidates = set()\n",
    "\n",
    "        for doc, score in results:\n",
    "            filename = os.path.basename(doc.metadata.get('source', ''))\n",
    "            if filename in self.resume_metadata:\n",
    "                candidate_info = self.resume_metadata[filename]['candidate_info']\n",
    "                \n",
    "                # Create unique identifier for candidate\n",
    "                candidate_id = f\"{candidate_info['Email']}_{candidate_info['Phone']}\"\n",
    "                \n",
    "                if candidate_id in seen_candidates:\n",
    "                    continue\n",
    "                    \n",
    "                seen_candidates.add(candidate_id)\n",
    "                \n",
    "                # Extract candidate skills with categories\n",
    "                candidate_skills = self._extract_skills_from_text(doc.page_content)\n",
    "                \n",
    "                # Calculate matches and gaps for each skill category\n",
    "                skill_matches = {\n",
    "                    'technical': {\n",
    "                        'matching': candidate_skills['technical_skills'].intersection(required_skills['technical_skills']),\n",
    "                        'missing': required_skills['technical_skills'] - candidate_skills['technical_skills'],\n",
    "                        'additional': candidate_skills['technical_skills'] - required_skills['technical_skills']\n",
    "                    },\n",
    "                    'soft': {\n",
    "                        'matching': candidate_skills['soft_skills'].intersection(required_skills['soft_skills']),\n",
    "                        'missing': required_skills['soft_skills'] - candidate_skills['soft_skills'],\n",
    "                        'additional': candidate_skills['soft_skills'] - required_skills['soft_skills']\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                # Calculate match scores\n",
    "                technical_match = (len(skill_matches['technical']['matching']) / \n",
    "                                 len(required_skills['technical_skills'])) * 100 if required_skills['technical_skills'] else 0\n",
    "                soft_match = (len(skill_matches['soft']['matching']) / \n",
    "                            len(required_skills['soft_skills'])) * 100 if required_skills['soft_skills'] else 0\n",
    "                \n",
    "                # Overall match score (weighted)\n",
    "                match_score = (technical_match * 0.7) + (soft_match * 0.3)\n",
    "                \n",
    "                matches.append({\n",
    "                    'filename': filename,\n",
    "                    'name': candidate_info['Full Name'],\n",
    "                    'email': candidate_info['Email'],\n",
    "                    'phone': candidate_info['Phone'],\n",
    "                    'current_position': candidate_info['Current Position'],\n",
    "                    'experience': candidate_info['Years of Experience'],\n",
    "                    'education': candidate_info['Education'],\n",
    "                    'technical_skills': {\n",
    "                        'matching': list(skill_matches['technical']['matching']),\n",
    "                        'missing': list(skill_matches['technical']['missing']),\n",
    "                        'additional': list(skill_matches['technical']['additional'])\n",
    "                    },\n",
    "                    'soft_skills': {\n",
    "                        'matching': list(skill_matches['soft']['matching']),\n",
    "                        'missing': list(skill_matches['soft']['missing']),\n",
    "                        'additional': list(skill_matches['soft']['additional'])\n",
    "                    },\n",
    "                    'match_score': round(match_score, 2),\n",
    "                    'technical_match': round(technical_match, 2),\n",
    "                    'soft_match': round(soft_match, 2)\n",
    "                })\n",
    "\n",
    "        matches.sort(key=lambda x: x['match_score'], reverse=True)\n",
    "        return matches\n",
    "\n",
    "    def _extract_skills_from_text(self, text: str) -> Dict[str, Set[str]]:\n",
    "        \"\"\"Extract skills from text with categorization\"\"\"\n",
    "        text = text.lower()\n",
    "        found_skills = {\n",
    "            'technical_skills': set(),\n",
    "            'soft_skills': set(),\n",
    "            'domain_skills': set()\n",
    "        }\n",
    "        \n",
    "        # Process text with spaCy\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        # Extract noun phrases and individual tokens\n",
    "        potential_skills = set()\n",
    "        for chunk in doc.noun_chunks:\n",
    "            potential_skills.add(chunk.text.lower())\n",
    "        for token in doc:\n",
    "            if not token.is_stop and not token.is_punct:\n",
    "                potential_skills.add(token.text.lower())\n",
    "        \n",
    "        # Match against skills dictionary\n",
    "        for skill_type, skills in self.skills_dict.items():\n",
    "            for skill in skills:\n",
    "                # Check for exact matches\n",
    "                if skill in text:\n",
    "                    if skill_type == 'soft_skills':\n",
    "                        found_skills['soft_skills'].add(skill)\n",
    "                    else:\n",
    "                        found_skills['technical_skills'].add(skill)\n",
    "                \n",
    "                # Check for variations (e.g., \"Python programming\", \"Python developer\")\n",
    "                for potential in potential_skills:\n",
    "                    if skill in potential and len(skill) > 2:  # Avoid matching too short strings\n",
    "                        if skill_type == 'soft_skills':\n",
    "                            found_skills['soft_skills'].add(skill)\n",
    "                        else:\n",
    "                            found_skills['technical_skills'].add(skill)\n",
    "        \n",
    "        return found_skills\n",
    "\n",
    "    def _calculate_experience_score(self, experience: str) -> float:\n",
    "        try:\n",
    "            if isinstance(experience, str):\n",
    "                years = float(re.findall(r'\\d+', experience)[0])\n",
    "            else:\n",
    "                years = float(experience)\n",
    "            return min(100, years * 20)\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def get_candidate_info(self, filename: str) -> Dict:\n",
    "        if filename not in self.resume_metadata:\n",
    "            return {\n",
    "                \"error\": f\"No candidate found with filename: {filename}\",\n",
    "                \"status\": \"error\"\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"candidate_info\": self.resume_metadata[filename]['candidate_info']\n",
    "        }\n",
    "\n",
    "    def analyze_skills_gap(self, filename: str, job_description: str) -> Dict:\n",
    "        if filename not in self.resume_metadata:\n",
    "            return {\n",
    "                \"error\": f\"No candidate found with filename: {filename}\",\n",
    "                \"status\": \"error\"\n",
    "            }\n",
    "\n",
    "        candidate_info = self.resume_metadata[filename]['candidate_info']\n",
    "        job_skills = self._extract_skills_from_text(job_description)\n",
    "        candidate_skills = set(skill.lower() for skill in candidate_info['Key Skills'])\n",
    "\n",
    "        matching_skills = candidate_skills.intersection(job_skills)\n",
    "        missing_skills = job_skills - candidate_skills\n",
    "        additional_skills = candidate_skills - job_skills\n",
    "\n",
    "        match_percentage = (len(matching_skills) / len(job_skills) * 100) if job_skills else 0\n",
    "\n",
    "        analysis = {\n",
    "            \"status\": \"success\",\n",
    "            \"analysis\": {\n",
    "                \"candidate_name\": candidate_info['Full Name'],\n",
    "                \"match_percentage\": round(match_percentage, 2),\n",
    "                \"matching_skills\": list(matching_skills),\n",
    "                \"missing_skills\": list(missing_skills),\n",
    "                \"additional_skills\": list(additional_skills),\n",
    "                \"total_required_skills\": len(job_skills),\n",
    "                \"total_matching_skills\": len(matching_skills),\n",
    "                \"total_missing_skills\": len(missing_skills)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def run(self, query: str, max_retries: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        Execute a query with retry mechanism.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user's query.\n",
    "            max_retries (int): Maximum number of retries if the query fails.\n",
    "\n",
    "        Returns:\n",
    "            str: The final answer or an error message.\n",
    "        \"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"Attempt {attempt + 1} to process query: {query}\")\n",
    "\n",
    "                # Extract required skills from the query\n",
    "                required_skills = self._extract_skills_from_text(query)\n",
    "                if required_skills:\n",
    "                    query = f\"{query} (Required skills identified: {', '.join(required_skills)})\"\n",
    "\n",
    "                # Execute the agent\n",
    "                response = self.agent_executor.invoke({\"input\": query})\n",
    "\n",
    "                # Check if the response is satisfactory\n",
    "                if self._is_response_satisfactory(response):\n",
    "                    return response[\"output\"] if \"output\" in response else str(response)\n",
    "                else:\n",
    "                    print(\"Response not satisfactory. Retrying...\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing query: {str(e)}. Retrying...\")\n",
    "\n",
    "        # If all retries fail, return an error message\n",
    "        return f\"Failed to process query after {max_retries} attempts. Please try again later.\"\n",
    "\n",
    "    def _is_response_satisfactory(self, response: dict) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the agent's response is satisfactory.\n",
    "\n",
    "        Args:\n",
    "            response (dict): The agent's response.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the response is satisfactory, False otherwise.\n",
    "        \"\"\"\n",
    "        # Example: Check if the response contains valid data\n",
    "        if \"output\" in response and response[\"output\"]:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
